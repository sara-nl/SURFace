{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pytz\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from plotnine import *\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dataset = \"path to machine metric dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of physical cores in LISA so that we can compare it with the summed load1 metric across all nodes.\n",
    "# Based on https://userinfo.surfsara.nl/systems/lisa/description accessed on 2020-10-08\n",
    "number_of_cores_in_lisa = 0\n",
    "number_of_cores_in_lisa += 23  * 6  # Intel® Xeon® Bronze 3104 Processor \n",
    "number_of_cores_in_lisa += 2   * 6  # Intel® Xeon® Bronze 3104 Processor \n",
    "number_of_cores_in_lisa += 29  * 12 # Intel® Xeon® Gold 5118 Processor \n",
    "number_of_cores_in_lisa += 192 * 16 # Intel® Xeon® Gold 6130 Processor \n",
    "number_of_cores_in_lisa += 96  * 8  # Intel® Xeon® Silver 4110 Processor \n",
    "number_of_cores_in_lisa += 1   * 12 # Intel® Xeon® Gold 6126 Processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the load1 computation\n",
    "load1_df = pd.read_parquet(os.path.join(root_dataset, \"node_load1\"))\n",
    "# load1_df[\"dt\"] = pd.to_datetime(df_1128.index, utc=True, unit=\"s\")\n",
    "# load1_df[\"dt\"] = load1_df[\"dt\"].dt.tz_convert(pytz.timezone('Europe/Amsterdam')).dt.tz_localize(None)\n",
    "# load1_df = load1_df.set_index(\"dt\")\n",
    "load1_df.replace(-1.0, 0, inplace=True) # Make sure we set \"invalid values\" to 0 to avoid influencing the sum\n",
    "load1_df = load1_df.sum(axis=1, skipna=True) # Sum the values across the columns (axis = 1), so we get per timestamp the total load1 across LISA\n",
    "\n",
    "load1_df = load1_df / number_of_cores_in_lisa\n",
    "load1_df.clip(upper=1.0, inplace=True)  # Clip values above the number of cores to 1 ()\n",
    "load1_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1577685615"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load1_df.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33402266523285634"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tempature_allowed =  77  # based on https://ark.intel.com/content/www/us/en/ark/products/123547/intel-xeon-silver-4110-processor-11m-cache-2-10-ghz.html which has the lowest T_CASE across all\n",
    "\n",
    "# Perform the ambient temperature computation\n",
    "rack_temp = pd.read_parquet(os.path.join(root_dataset, \"surfsara_ambient_temp\"))\n",
    "rack_temp.replace(-1.0, np.nan, inplace=True)  # Set -1 values which are invalid to NaNs so we can omit them.\n",
    "rack_temp = rack_temp.mean(axis=1, skipna=True)  # Compute the mean, skip NaNs\n",
    "rack_temp.dropna(how='all', inplace=True)  # For safety: drop timestamps that had 0 valid values.\n",
    "rack_temp = rack_temp / max_tempature_allowed\n",
    "rack_temp.clip(upper=1.0, inplace=True)  # Clip values above 1 with respect to max_tempature_allowed\n",
    "rack_temp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISA has two types of GPUs:\n",
    "# GeForce 1080Ti and Titan V. Both have the Thermal Threshold of 91 degree Celcius.\n",
    "# https://www.nvidia.com/en-sg/geforce/products/10series/geforce-gtx-1080-ti/\n",
    "# https://www.nvidia.com/en-us/titan/titan-v/\n",
    "max_gpu_tempature_allowed =  91\n",
    "\n",
    "# Perform the ambient temperature computation\n",
    "gpu_temp = pd.read_parquet(os.path.join(root_dataset, \"nvidia_gpu_temperature_celsius\"))\n",
    "gpu_temp.replace(-1.0, np.nan, inplace=True)  # Set -1 values which are invalid to NaNs so we can omit them.\n",
    "gpu_temp = gpu_temp.mean(axis=1, skipna=True)  # Compute the mean, skip NaNs\n",
    "gpu_temp.dropna(how='all', inplace=True)  # For safety: drop timestamps that had 0 valid values.\n",
    "gpu_temp = gpu_temp / max_gpu_tempature_allowed\n",
    "gpu_temp.clip(upper=1.0, inplace=True)  # Clip values above 1 with respect to max_tempature_allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8290666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The average power consumption throught the datacenter.\n",
    "# The cooling for all the racks is the same, with a capability of max 5500 Watts\n",
    "\n",
    "cluster_power_df = pd.read_parquet(os.path.join(root_dataset, \"surfsara_power_usage\"))\n",
    "\n",
    "cluster_power_df.replace(-1.0, np.nan, inplace=True)  # Set -1 values which are invalid to NaNs so we can omit them.\n",
    "\n",
    "# Drop columns that only have NaN values, we need to drop them because they will add to max_allowed_power_consumption but do not contribute in the sum.\n",
    "cluster_power_df.dropna(how='all', axis=1, inplace=True) \n",
    "cluster_power_df.dropna(how='all', inplace=True)  # For safety: drop timestamps that had 0 valid values.\n",
    "\n",
    "# Lisa's cooling system can handle 5500 Watt per rack, compute this ceiling.\n",
    "max_allowed_power_consumption = 5500 * len({col.split(\"n\")[0] for col in cluster_power_df.columns})\n",
    "\n",
    "cluster_power_df = cluster_power_df.sum(axis=1, skipna=True)  # Compute the mean, skip NaNs\n",
    "cluster_power_df = cluster_power_df / max_allowed_power_consumption\n",
    "cluster_power_df.clip(upper=1.0, inplace=True)  # Clip values above 1 as we know values can exceed the cooling system's capability.\n",
    "cluster_power_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1577685615    0.137794\n",
      "1577685630    0.145134\n",
      "1577685645    0.145976\n",
      "1577685660    0.145503\n",
      "1577685675    0.145419\n",
      "                ...   \n",
      "1589925525    0.138196\n",
      "1589925540    0.138075\n",
      "1589925555    0.137709\n",
      "1589925570    0.137474\n",
      "1589925585    0.137419\n",
      "Length: 804719, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RAM usage computation.\n",
    "memory_total = pd.read_parquet(os.path.join(root_dataset, \"node_memory_MemTotal\"))\n",
    "memory_free = pd.read_parquet(os.path.join(root_dataset, \"node_memory_MemFree\"))\n",
    "\n",
    "# Align the two dataframes, use inner so that all mismatched between columns and timestmaps are dropped.\n",
    "# Axis = none aligns on both index AND columns, which is what we want.\n",
    "memory_total, memory_free = memory_total.align(memory_free, join=\"inner\", axis=None)\n",
    "\n",
    "# We can now devide safely because all indices and columns are aligned.\n",
    "memory_df = memory_free / memory_total\n",
    "del memory_total, memory_free # Free up RAM during computation\n",
    "\n",
    "# Switch to memory_consumed rather than free\n",
    "memory_df = 1 - memory_df\n",
    "\n",
    "# Compute the average\n",
    "memory_df = memory_df.mean(axis=1, skipna=True)\n",
    "print(memory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5461344537815126"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU fan speed computation\n",
    "max_fanspeed =  100  # It's percentage based \n",
    "\n",
    "gpu_fanspeed_df = pd.read_parquet(os.path.join(root_dataset, \"nvidia_gpu_fanspeed_percent\"))\n",
    "gpu_fanspeed_df.replace(-1.0, np.nan, inplace=True)  # Set -1 values which are invalid to NaNs so we can omit them.\n",
    "gpu_fanspeed_df = gpu_fanspeed_df.mean(axis=1, skipna=True)  # Compute the mean, skip NaNs\n",
    "gpu_fanspeed_df.dropna(how='all', inplace=True)  # For safety: drop timestamps that had 0 valid values.\n",
    "gpu_fanspeed_df = gpu_fanspeed_df / max_fanspeed\n",
    "gpu_fanspeed_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.519530961007783"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU memory computation\n",
    "gpu_memory_df = pd.read_parquet(os.path.join(root_dataset, \"nvidia_gpu_memory_used_bytes\"))\n",
    "\n",
    "# # Here, we need to do a bit of computation.\n",
    "# # There are three GPU versions in the cluster, the GTX 1080 ti with 11GB or RAM and the Titan V / Titan RTX, which have 12 and 24GB of RAM.\n",
    "# # We first ceil the maximum usage per node and ceil it. Then per rack, we find the maximum value and set that maximum to all nodes.\n",
    "# # If the maximum is >11.0 then it must be a Titan with 24GB. If the value is <= 11.0 then we assume it's the 1080 ti.\n",
    "# # This means that a poorly used Titans across its entire lifespan (which is possibly, but unlikely) will be classified as a 1080ti, but \n",
    "# # as this is highly unlikely, we beleive this is for now the best approach given the data.\n",
    "# ceiled_values = np.ceil((gpu_memory_df.max() / (1024*1024*1024)))\n",
    "\n",
    "# # Based on https://github.com/sara-nl/SURFace/blob/master/node%20information.md\n",
    "\n",
    "# node_max = dict()\n",
    "# for index, value in ceiled_values.items():\n",
    "#     if value > 12.0 or index in confirmed_titan_nodes:\n",
    "#         value = 24.0\n",
    "#     elif value > 11.0:\n",
    "#         value = 12.0\n",
    "#     else:\n",
    "#         value = 11.0\n",
    "    \n",
    "#     if index not in node_max:\n",
    "#         node_max[index] = value\n",
    "    \n",
    "#     node_max[index] = max(node_max[index], value)\n",
    "\n",
    "# print(node_max)\n",
    "\n",
    "# # gpu_memory_df.replace(-1.0, np.nan, inplace=True)  # Set -1 values which are invalid to NaNs so we can omit them.\n",
    "\n",
    "# # Devide all values based on the found maximum.\n",
    "# for node_name, max_gpu_value in node_max.items():\n",
    "#     gpu_memory_df[node_name] = gpu_memory_df[node_name] / (max_gpu_value * 1024 * 1024 * 1024)\n",
    "\n",
    "for column in gpu_memory_df.columns:\n",
    "    gpu_name = column[1]\n",
    "    if \"GeForce GTX 1080 Ti\" in gpu_name:\n",
    "        GPU_GB = 11\n",
    "    elif \"TITAN V\" in gpu_name:\n",
    "        GPU_GB = 12\n",
    "    elif \"TITAN RTX\" in gpu_name:\n",
    "        GPU_GB = 24\n",
    "    else:\n",
    "        print(gpu_name)\n",
    "        GPU_GB = 0  # Make it crash - unknown GPU type\n",
    "    gpu_memory_df[column] = gpu_memory_df[column] / (GPU_GB * 1024 * 1024 * 1024)\n",
    "    \n",
    "gpu_memory_df.dropna(how='all', inplace=True)  # For safety: drop timestamps that had 0 valid values.\n",
    "gpu_memory_df = gpu_memory_df.mean(axis=1, skipna=True)  # Compute the mean across the entire cluster for all nodes,gpu pairs\n",
    "gpu_memory_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387104665.6\n",
      "867582939511.4667\n",
      "867583387852.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DISK IO\n",
    "\n",
    "# For this, we are summing up the node_disk_bytes_read values and the node_disk_bytes_written\n",
    "bytes_read_df = pd.read_parquet(os.path.join(root_dataset, \"node_disk_bytes_read\"))\n",
    "bytes_read_df.replace(-1.0, np.nan, inplace=True)\n",
    "\n",
    "# Compute the time since the last timestamp, in case there are gaps in the index.\n",
    "time_between_indices = bytes_read_df.index.to_series().diff().dropna()\n",
    "\n",
    "# Drop columns with no recorded values\n",
    "bytes_read_df.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "# As the values are incremental, we have totake the difference between the row and the previous\n",
    "bytes_read_df = bytes_read_df.diff()\n",
    "\n",
    "# Drop rows with only NA values\n",
    "bytes_read_df.dropna(how='all', inplace=True)  # The first row will be N/A because we do not know how many bytes were writting on that timestamp, there may be others.\n",
    "\n",
    "# Convert the bytes_read to values per second\n",
    "bytes_read_df = bytes_read_df.div(time_between_indices, axis=0)\n",
    "print(bytes_read_df.max().max())\n",
    "\n",
    "# We apply the same to bytes_written\n",
    "bytes_written_df = pd.read_parquet(os.path.join(root_dataset, \"node_disk_bytes_written\"))\n",
    "bytes_written_df.replace(-1.0, np.nan, inplace=True)\n",
    "\n",
    "# Compute the time since the last timestamp, in case there are gaps.\n",
    "time_between_indices = bytes_written_df.index.to_series().diff().dropna()\n",
    "\n",
    "# Drop columns with no recorded values\n",
    "bytes_written_df.dropna(how='all', axis=1, inplace=True) \n",
    "bytes_written_df = bytes_written_df.diff()\n",
    "bytes_written_df.dropna(how='all', inplace=True)\n",
    "bytes_written_df = bytes_written_df.div(time_between_indices, axis=0)\n",
    "print(bytes_written_df.max().max())\n",
    "\n",
    "del time_between_indices\n",
    "\n",
    "# Align the two dataframes on index and columns (for safety) so we can safely sum!\n",
    "bytes_read_df, bytes_written_df = bytes_read_df.align(bytes_written_df, join=\"inner\", axis=None)\n",
    "\n",
    "# Sum them up!\n",
    "disk_io_df = bytes_read_df + bytes_written_df\n",
    "del bytes_read_df, bytes_written_df  # Free up RAM\n",
    "\n",
    "print(disk_io_df.max().max())\n",
    "\n",
    "# Compute how much IO we can expect, assuming that all the nodes have a disk that we investigate\n",
    "max_disk_io = 1.8 * disk_io_df.shape[1] * 1000 * 1000 * 1000\n",
    "disk_io_df = disk_io_df.sum(axis=1, skipna=True) # Sum the values across the columns (axis = 1), so we get per timestamp the total load1 across LISA\n",
    "disk_io_df = disk_io_df / max_disk_io # Get the fractions, and we're done!\n",
    "\n",
    "disk_io_df.clip(upper=1.0, inplace=True)  # Clip values above the number of cores to 1 ()\n",
    "\n",
    "disk_io_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Submit\n",
       "1577477610    1.000000\n",
       "1577477625    0.000000\n",
       "1577477640    0.000000\n",
       "1577477655    0.000000\n",
       "1577477670    0.000000\n",
       "                ...   \n",
       "1596758145    0.000000\n",
       "1596758160    0.000000\n",
       "1596758175    0.000000\n",
       "1596758190    0.000000\n",
       "1596758205    0.111111\n",
       "Name: JobID, Length: 1285374, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of incoming jobs\n",
    "# Some of the code in this cell comes from Caspar Greeven at SURFsara \n",
    "\n",
    "# Required preprocessing/parsing of the job data\n",
    "def preprocess_jobdata_to_df(name):\n",
    "    with open(os.path.join(location_job_data_csv, name),'r') as file:\n",
    "        filedata = file.read()\n",
    "        filedata = filedata.replace('None assigned','NoneAssigned')\n",
    "    with open(os.path.join(location_job_data_csv, str('processed_'+name)),'w') as file:\n",
    "        file.write(filedata)\n",
    "    jobdata = pd.read_fwf(os.path.join(location_job_data_csv, str('processed_'+name)), delimiter=r\"\\s+\", header=None)#, low_memory=False)\n",
    "    jobdata = jobdata.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    jobdata = jobdata.rename(columns=jobdata.iloc[0]).drop(jobdata.index[0])\n",
    "    jobdata = jobdata.iloc[1:]\n",
    "    jobdata = jobdata.astype({\"ElapsedRaw\": int, \"CPUTimeRAW\": int, \"NCPUS\": int})\n",
    "    return(jobdata)\n",
    "\n",
    "location_job_data_csv = \"path to machine workload\"\n",
    "job_arrival_df = preprocess_jobdata_to_df(\"jobdata.csv\")\n",
    "job_arrival_df = job_arrival_df[(job_arrival_df[\"Start\"] >= '2019-12-29 23:00:00') & (job_arrival_df[\"Start\"] <= '2020-08-07 21:59:45')]\n",
    "job_arrival_df = job_arrival_df[(~job_arrival_df[\"NodeList\"].str.contains(\"None\")) & (~job_arrival_df[\"NodeList\"].str.contains(\"software\")) & (~job_arrival_df[\"NodeList\"].str.contains(\"login\"))]\n",
    "job_arrival_df[\"Submit\"] = pd.to_datetime(job_arrival_df[\"Submit\"], utc=True, format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "job_arrival_df = job_arrival_df.set_index(\"Submit\")\n",
    "job_arrival_df = job_arrival_df['JobID'].resample(\"15S\").count()  # Just grab one random column (JobID) to count\n",
    "# Convert the datetime index back to seconds\n",
    "job_arrival_df.index = job_arrival_df.index.astype(np.int64) // 10 ** 9\n",
    "\n",
    "# We observed one gigantic peak in the dataset. Normalizing against that extreme outlier \"dwarfs\" the other periods and \n",
    "# masks the overall behavior. Instead, we normalize against the 99th percentile to avoid these extreme outliers and cap the values to 1\n",
    "job_arrival_per_15s_99th_percentile = job_arrival_df.quantile(0.99)\n",
    "job_arrival_df /= job_arrival_per_15s_99th_percentile\n",
    "job_arrival_df.clip(upper=1.0, inplace=True)  # Clip values to 1 as we do not normalize against the max\n",
    "\n",
    "job_arrival_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU power computation\n",
    "gpu_power_df = pd.read_parquet(os.path.join(root_dataset, \"nvidia_gpu_power_usage_milliwatts\"))\n",
    "\n",
    "for column in gpu_power_df.columns:\n",
    "    gpu_name = column[1]\n",
    "    if \"GeForce GTX 1080 Ti\" in gpu_name:\n",
    "        GPU_TDP = 250  # TDP see https://www.geforce.com/hardware/desktop-gpus/geforce-gtx-1080-ti/specifications\n",
    "    elif \"TITAN V\" in gpu_name:\n",
    "        GPU_TDP = 250  # TDP see https://www.nvidia.com/en-us/titan/titan-v/ \n",
    "    elif \"TITAN RTX\" in gpu_name:\n",
    "        GPU_TDP = 280  # TDP see https://www.nvidia.com/en-us/deep-learning-ai/products/titan-rtx/\n",
    "    else:\n",
    "        print(gpu_name)\n",
    "        GPU_TDP = 0  # Make it crash - unknown GPU type\n",
    "    gpu_power_df[column] = gpu_power_df[column] / GPU_TDP / 1000  # 1000 because we want Watts\n",
    "    \n",
    "gpu_power_df.dropna(how='all', inplace=True)  # For safety: drop timestamps that had 0 valid values.\n",
    "gpu_power_df = gpu_power_df.mean(axis=1, skipna=True)  # Compute the mean across the entire cluster for all nodes,gpu pairs\n",
    "gpu_power_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the overview figure\n",
    "\n",
    "series_to_use = [\n",
    "    (job_arrival_df, \"Job Arrivals\"),\n",
    "    (load1_df, \"CPU Load\"),\n",
    "    (cluster_power_df, \"Server Power Usage\"),\n",
    "    (rack_temp, \"Server Temperature\"),\n",
    "    (memory_df, \"Server RAM Usage\"),\n",
    "    (disk_io_df, \"Disk I/O\"),\n",
    "    (gpu_power_df, \"GPU Power Usage\"),\n",
    "    (gpu_temp, \"GPU Temperature\"),\n",
    "    (gpu_fanspeed_df, \"GPU Fan Speed\"),\n",
    "    (gpu_memory_df, \"GPU Memory Usage\"),\n",
    "]\n",
    "\n",
    "# for ser, name in series_to_use:\n",
    "#     ser.name = name\n",
    "    \n",
    "# df = pd.concat([ser for ser, _ in series_to_use], axis=1)\n",
    "\n",
    "df = None\n",
    "\n",
    "for ser, name in series_to_use:\n",
    "    ser.name = name\n",
    "    if df is None:\n",
    "        df = ser.to_frame()\n",
    "    else:\n",
    "        df = df.merge(ser, how='outer', left_index=True, right_index=True)\n",
    "    #del ser  # Free up RAM\n",
    "\n",
    "# Safety, make sure the timestamps are in order\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Important! Get the start now, because we are going to apply a window and that the right side (end)\n",
    "start = df.index[0]\n",
    "\n",
    "# df.dropna(how='all', axis='index', inplace=True)  # Drop rows with only NaNs  - this will remove gaps but ma\n",
    "df.fillna(0.0, inplace=True) # Fill in others with 0 so they show up as not significant at least\n",
    "\n",
    "# Convert the index to a date so we can resample using a 1 hour window.\n",
    "df[\"dt\"] = pd.to_datetime(df.index, utc=True, unit=\"s\")\n",
    "df[\"dt\"] = df[\"dt\"].dt.tz_convert(pytz.timezone('Europe/Amsterdam')).dt.tz_localize(None)\n",
    "df = df.set_index(\"dt\")\n",
    "df = df.resample('1H', label='right').max()  # For each hour, get the maximum\n",
    "df.index = df.index.astype(np.int64) // 10**9  # Convert back to unix timestamp so we can modify the plot axis later\n",
    "\n",
    "cmap = plt.cm.get_cmap('Reds')\n",
    "# cNorm = plt.cm.colors.Normalize(vmin=0, vmax=1)  # Use this one if you want a linear gradient\n",
    "cNorm = mpl.colors.BoundaryNorm(np.linspace(0, 1, 6), cmap.N)  # Use this one if you want a discrete gradient\n",
    "scalarMap = plt.cm.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17, 4))\n",
    "ax.invert_yaxis()\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "ax.set_xlabel(\"Time [Hour]\", fontsize=18)\n",
    "\n",
    "#https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "left = 0\n",
    "for index, row in df.iterrows():  # Iterate over all the rows\n",
    "    width = index - (start+left)  # Get how wide this piece should be, based on the previous value (timestamp) to account for big gaps\n",
    "    lefts = [left] * len(df.columns)  # For all values, set the offset\n",
    "    colors = row.apply(scalarMap.to_rgba)  # Compute the colors for this particular slice\n",
    "    ax.barh(df.columns, width=width, left=lefts, height=1.0, linewidth=0, label=df.columns, color=colors, rasterized=True)\n",
    "    left += width  # update left\n",
    "\n",
    "def get_converted_xticks(ax):\n",
    "    \"\"\"\n",
    "    :param ax:\n",
    "    :return list of day and month strings\n",
    "    \"\"\"\n",
    "    # return [pd.to_datetime(start + tick, unit='s').date().strftime(\"%d\\n%b\") for tick in ax.get_xticks()]\n",
    "    return [int((tick) / 3600) for tick in ax.get_xticks()]  # Get the hour number\n",
    "\n",
    "ax.set_xticks(np.linspace(0, ax.get_xlim()[1], num=6, dtype=int))  # Make sure we include 0 and the last value in the plot, so set the xticks ourselves\n",
    "ax.set_xticklabels(get_converted_xticks(ax))\n",
    "\n",
    "\n",
    "# Draw a line at the start of each month + annotate the date\n",
    "first_line = pd.to_datetime(df.index[0], unit='s').date()\n",
    "cur_year = first_line.year\n",
    "cur_month = first_line.month\n",
    "cur_month += 1\n",
    "if cur_month == 13:\n",
    "    cur_month = 1\n",
    "    cur_year += 1\n",
    "start_next_month = time.mktime(datetime(cur_year, cur_month, 1).timetuple())\n",
    "while start_next_month <= df.index[-1]:\n",
    "    ax.axvline(start_next_month - start, ymin=0.05, ymax=0.95, color='black', linestyle='dashed')  # seems we have some margin due to hbars?\n",
    "    ax.text(x=start_next_month - start, y=-0.7, s=datetime(cur_year, cur_month, 1).strftime(\"%B\"), fontsize=16, ha=\"center\")\n",
    "    cur_month += 1\n",
    "    if cur_month == 13:\n",
    "        cur_month = 1\n",
    "        cur_year += 1\n",
    "    start_next_month = time.mktime(datetime(cur_year, cur_month, 1).timetuple())\n",
    "\n",
    "# Based on https://stackoverflow.com/a/39938019 and https://stackoverflow.com/a/39938019\n",
    "pos = ax.get_position()\n",
    "pad = 0.01\n",
    "width = 0.02\n",
    "ax2 = fig.add_axes([pos.xmax + pad, pos.ymin + 0.04, width, 0.9 * (pos.ymax - pos.ymin)])  # x, y, width, height\n",
    "fig.colorbar(scalarMap, cax=ax2)\n",
    "fig.subplots_adjust(bottom=0.15)\n",
    "\n",
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "fig.savefig(f\"generic_bottleneck_surf_{date_time}.pdf\", bbox_inches = \"tight\")\n",
    "fig.savefig(f\"generic_bottleneck_surf_{date_time}.png\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1589508000 in df.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
