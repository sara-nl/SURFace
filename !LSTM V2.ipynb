{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parquet_to_df(name):\n",
    "    df = pd.read_parquet(name)\n",
    "    df = df.replace(-1, 0)\n",
    "    df = df.fillna(0)\n",
    "    df[\"dt\"] = pd.to_datetime(df.index, utc=True, unit=\"s\")\n",
    "    df[\"dt\"] = df[\"dt\"].dt.tz_convert(pytz.timezone('Europe/Amsterdam')).dt.tz_localize(None)\n",
    "    df = df.set_index(\"dt\")\n",
    "    df = df.sort_index()\n",
    "    #df['Total']= df.sum(axis=1)\n",
    "    return(df)\n",
    "\n",
    "def resample_normalize_dataframe(df, window):\n",
    "    df = df.resample(window).mean()\n",
    "    df = (df-df.mean())/df.std()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all metric folders containing parquet files, select one parquet file for the indices (node names)\n",
    "# For each node a seperate parquet file is generated, containing only the metrics for this specific node\n",
    "folders = os.listdir(\"data\")\n",
    "\n",
    "index_df = preprocess_parquet_to_df(\"data/node_load1\")\n",
    "output_df = pd.DataFrame(index=index_df.index)\n",
    "\n",
    "for col in ['r10n13', 'r11n18', 'r12n6', 'r13n23', 'r14n2', 'r15n23', 'r25n32',\n",
    "            'r26n8', 'r27n16', 'r29n3', 'r30n6', 'r31n5', 'r38n1']: #index_df.columns:\n",
    "    print(col)\n",
    "    for f in folders:\n",
    "        if not f.startswith('.'):\n",
    "            #print(f)\n",
    "            # preprocess data\n",
    "            data = preprocess_parquet_to_df(\"data/\"+f)\n",
    "            if col in data.columns:\n",
    "                output_df[f] = data[col]#[['Total']].copy()    \n",
    "    output_df.to_parquet(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_node_df(file):\n",
    "    # Load in processed parquet for sample nodes\n",
    "    df = pd.read_parquet(file)\n",
    "    # Normalize data, resample to different time bins\n",
    "    df_15sec = (df-df.mean())/df.std()\n",
    "    df_1min = resample_normalize_dataframe(df,\"1T\")\n",
    "    df_5min = resample_normalize_dataframe(df,\"5T\")\n",
    "    df_10min = resample_normalize_dataframe(df,\"10T\")\n",
    "    return(df_15sec, df_1min, df_5min, df_10min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r10n13_15sec, r10n13_1min, r10n13_5min, r10n13_10min = process_node_df('r10n13')\n",
    "r11n18_15sec, r11n18_1min, r11n18_5min, r11n18_10min = process_node_df('r11n18')\n",
    "r12n6_15sec, r12n6_1min, r12n6_5min, r12n6_10min = process_node_df('r12n6')\n",
    "r13n23_15sec, r13n23_1min, r13n23_5min, r13n23_10min = process_node_df('r13n23')\n",
    "r14n2_15sec, r14n2_1min, r14n2_5min, r14n2_10min = process_node_df('r14n2')\n",
    "r15n23_15sec, r15n23_1min, r15n23_5min, r15n23_10min = process_node_df('r15n23')\n",
    "r25n32_15sec, r25n32_1min, r25n32_5min, r25n32_10min = process_node_df('r25n32')\n",
    "r26n8_15sec, r26n8_1min, r26n8_5min, r26n8_10min = process_node_df('r26n8')\n",
    "r27n16_15sec, r27n16_1min, r27n16_5min, r27n16_10min = process_node_df('r27n16')\n",
    "r29n3_15sec, r29n3_1min, r29n3_5min, r29n3_10min = process_node_df('r29n3')\n",
    "r30n6_15sec, r30n6_1min, r30n6_5min, r30n6_10min = process_node_df('r30n6')\n",
    "r31n5_15sec, r31n5_1min, r31n5_5min, r31n5_10min = process_node_df('r31n5')\n",
    "r38n1_15sec, r38n1_1min, r38n1_5min, r38n1_10min = process_node_df('r38n1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_series(series, n_past, n_future):\n",
    "    # n_past = no of past observations\n",
    "    # n_future = no of future observations \n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        # Slicing the past and future parts of the window\n",
    "        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - 2 hours input for all time granularities\n",
    "# Predict 20 minutes ahead\n",
    "# Compare loss values\n",
    "# Set parameters\n",
    "n_past = 480 # 480*15 seconds = 2 hours\n",
    "n_future = 80 # 80*15 seconds = 20 minutes\n",
    "n_features = 2 # ['node_sockstat_sockets_used', 'node_load1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two correlated metrics\n",
    "selection = r10n13_15sec[['node_sockstat_sockets_used', 'node_load1']].copy()\n",
    "# Split in train and evaluation datasets\n",
    "train, test = train_test_split(selection, test_size=0.10, shuffle=False)\n",
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset for training LSTM\n",
    "X_train, y_train = split_series(train.values,n_past, n_future)\n",
    "X_train = np.nan_to_num(X_train)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset for evaluating LSTM\n",
    "X_test, y_test = split_series(test.values,n_past, n_future)\n",
    "X_test = np.nan_to_num(X_train)\n",
    "y_test = np.nan_to_num(y_train)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\n",
    "y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model - based on https://www.analyticsvidhya.com/blog/2020/10/multivariate-multi-step-time-series-forecasting-using-stacked-lstm-sequence-to-sequence-autoencoder-in-tensorflow-2-0-keras/\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "encoder_l1 = tf.keras.layers.LSTM(100, return_state=True)\n",
    "encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "\n",
    "encoder_states1 = encoder_outputs1[1:]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_outputs1[0])\n",
    "\n",
    "decoder_l1 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "decoder_outputs1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l1)\n",
    "\n",
    "model = tf.keras.models.Model(encoder_inputs,decoder_outputs1)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Visualise LSTM\n",
    "dot_img_file = 'LSTM_V2.png'\n",
    "tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3)\n",
    "\n",
    "# Used for 15sec\n",
    "model_e1d1.fit(X_train,y_train,epochs=40,batch_size=5120,verbose=1,validation_split=0.20,\n",
    "               callbacks=[reduce_lr, earlystop_callback])\n",
    "\n",
    "# Used for 1min\n",
    "#model_e1d1.fit(X_train,y_train,epochs=40,batch_size=1280,verbose=1,validation_split=0.20,\n",
    "#               callbacks=[reduce_lr, earlystop_callback])\n",
    "\n",
    "# Used for 5min\n",
    "#model_e1d1.fit(X_train,y_train,epochs=40,batch_size=256,verbose=1,validation_split=0.20,\n",
    "#               callbacks=[reduce_lr, earlystop_callback])\n",
    "\n",
    "# Used for 10min\n",
    "#model_e1d1.fit(X_train,y_train,epochs=40,batch_size=128,verbose=1,validation_split=0.20,\n",
    "#               callbacks=[reduce_lr, earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM performance by comparing prediction to ground truth\n",
    "prediction = model.evaluate(X_test, y_test, batch_size=5120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
