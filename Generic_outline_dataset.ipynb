{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dataset = \"path to machine metric dataset\"\n",
    "folders = next(os.walk(root_dataset))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute generic data to be processed/printer later\n",
    "all_columns = set()\n",
    "max_number_of_samples_per_metric = 0\n",
    "total_number_metric_samples = 0\n",
    "total_number_of_valid_metric_samples = 0  # Metric samples minus NaNs in the data\n",
    "for folder in folders:\n",
    "    if \"nvidia\" in folder: continue  # Dask cannot handle multi Index dataframes\n",
    "        \n",
    "    # We use dask to lazy-load the datafame, only read as little as needed\n",
    "    try:\n",
    "        df = pd.read_parquet(os.path.join(root_dataset, folder), engine=\"pyarrow\")\n",
    "        all_columns.update(df.columns)\n",
    "        max_number_of_samples_per_metric = max(max_number_of_samples_per_metric, len(df))\n",
    "        total_metrics_in_dataset = df.shape[0] * df.shape[1]\n",
    "        total_number_metric_samples += total_metrics_in_dataset\n",
    "        total_number_of_valid_metric_samples += total_metrics_in_dataset - df.isnull().sum().sum()\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metrics in the dataset: 327\n",
      "Number of nodes in the dataset: 341\n",
      "Number of racks in the dataset: 20\n",
      "Maximum number of samples per metric: 1,258,646\n",
      "Total samples in the dataset: 66,541,895,243\n",
      "Total valid samples in the dataset: 63,978,689,791\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of metrics in the dataset: {len(folders)}\")\n",
    "print(f\"Number of nodes in the dataset: {len(all_columns)}\")\n",
    "print(f\"Number of racks in the dataset: {len(set([s.split('n')[0] for s in all_columns]))}\")\n",
    "print(f\"Maximum number of samples per metric: {'{:,}'.format(max_number_of_samples_per_metric)}\")\n",
    "print(f\"Total samples in the dataset: {'{:,}'.format(total_number_metric_samples)}\")\n",
    "print(f\"Total valid samples in the dataset: {'{:,}'.format(total_number_of_valid_metric_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r10',\n",
       " 'r11',\n",
       " 'r12',\n",
       " 'r13',\n",
       " 'r14',\n",
       " 'r15',\n",
       " 'r23',\n",
       " 'r25',\n",
       " 'r26',\n",
       " 'r27',\n",
       " 'r28',\n",
       " 'r29',\n",
       " 'r30',\n",
       " 'r31',\n",
       " 'r32',\n",
       " 'r33',\n",
       " 'r34',\n",
       " 'r35',\n",
       " 'r36',\n",
       " 'r38'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([s.split('n')[0] for s in all_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date_dataset = None\n",
    "max_date_dataset = None\n",
    "for folder in folders:\n",
    "    for file in next(os.walk(os.path.join(root_dataset, folder)))[2]:\n",
    "        start_time, end_time = file.replace(\".parquet\", \"\").split(\"_\")\n",
    "        min_date_dataset = int(start_time) if min_date_dataset is None else min(int(start_time), min_date_dataset)\n",
    "        max_date_dataset = int(end_time) if max_date_dataset is None else max(int(end_time), max_date_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min timestamp: {pd.to_datetime(min_date_dataset, unit='s')}, and max: {pd.to_datetime(max_date_dataset, unit='s')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(os.path.join(root_dataset, \"nvidia_gpu_memory_used_bytes/\"))\n",
    "\n",
    "print(df.columns)\n",
    "print(f\"Number of GPUs: {len(df.columns)}\")\n",
    "\n",
    "GPU_mem_in_GB = 0\n",
    "for node,gpu in df.columns:\n",
    "    if \"1080\" in gpu:\n",
    "        GPU_mem_in_GB += 11\n",
    "    elif \"TITAN RTX\" in gpu:\n",
    "        GPU_mem_in_GB += 24\n",
    "    else:\n",
    "        GPU_mem_in_GB += 12\n",
    "        \n",
    "print(f\"Total GPU memory: {GPU_mem_in_GB}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
